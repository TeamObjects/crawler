# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter
from urllib.request import urlretrieve
from urllib.error import HTTPError
import os
import csv
import time
import re
import datetime
from .items import CtgrInfoItem, ProductInfoDetailItem
from collections import defaultdict

class ScrapForBridgePipeline:
    def __init__(self):
        self.start_time = datetime.date.today().strftime("%y%m%d")
        self.make_dirs('../data')
        self.items_count = defaultdict(int)
        self.ctgr_info_file_name = os.path.join("../data/", f'ì¹´í…Œê³ ë¦¬ì •ë³´_{self.start_time}')
        self.item_info_file_name = os.path.join("../data/", f'ìƒí’ˆì •ë³´_{self.start_time}')
        
        #ì¹´í…Œê³ ë¦¬ ì •ë³´ tsv íŒŒì¼ ìƒì„±
        with open(f'{self.ctgr_info_file_name}.tsv', 'a',encoding='utf-8', newline='') as ctgr_info_file:
            writer = csv.writer(ctgr_info_file, delimiter='\t')
            writer.writerow(['1ëìŠ¤ ì´ë¦„', '2ëìŠ¤ ì´ë¦„','3ëìŠ¤ ì´ë¦„','3ëìŠ¤ url'])
        #ìƒí’ˆ ì •ë³´ tsv íŒŒì¼ ìƒì„±
        with open(f'{self.item_info_file_name}.tsv','a',encoding='utf-8', newline='') as item_info_file:
                writer2 = csv.writer(item_info_file, delimiter='\t')
                writer2.writerow(['ctgr1','ctgr2','ctgr3','product_name','main_img','item_img_path','dc_rate','price','tags_obj','detail_imgs'])
        
    def process_item(self, item, spider):
        # ì¹´í…Œê³ ë¦¬ ì •ë³´ ë„˜ì–´ì˜¬ ë•Œ ì‹¤í–‰
        if isinstance(item,CtgrInfoItem):
            print(f'processing, {item["ctgr3_name"]}')
            with open(f'{self.ctgr_info_file_name}.tsv', 'a',encoding='utf-8', newline='') as ctgr_info_file:
                writer = csv.writer(ctgr_info_file, delimiter='\t')
                writer.writerow([item['ctgr1_name'], item['ctgr2_name'],item['ctgr3_name'],item['third_depth_url']])
        
        # ìƒí’ˆ ì •ë³´ ì €ì¥ íŒŒì´í”„ë¼ì¸
        if isinstance(item,ProductInfoDetailItem):
            print(f'processing, {item["product_name"]}')
            img_urls = item['item_img_urls']
            product_name = self.change_word(item['product_name'])
            ctgr1 = item['ctgr1']
            ctgr2 = item['ctgr2']
            ctgr3 = item['ctgr3']
            coupang_prod_id = re.sub(r' \- ','_',item['tags_obj'][-1]['ì¿ íŒ¡ìƒí’ˆë²ˆí˜¸'])
            
            # download path ì„¤ì • (os ê²½ë¡œ+ category_path + ìƒí’ˆ title.replace("/","_").replace(".","_") + ìƒí’ˆid)
            img_folder_path = f"../data/{ctgr1}/{ctgr2}/{ctgr3}/{product_name}/{coupang_prod_id}"
            # ìƒí’ˆ ì´ë¯¸ì§€ ì €ì¥ í´ë” ìƒì„±
            self.make_dirs(img_folder_path)
            
            for i,img_url in enumerate(img_urls):
                # main ì´ë¯¸ì§€ì¼ ê²½ìš° ê²½ë¡œ ë‹¤ë¥´ê²Œ í•˜ê¸°
                if i == 0 :
                    #ìƒí’ˆ ìƒì„¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                    for detail_url in item['detail_urls']:
                        self.img_req(detail_url,self.make_dirs(img_folder_path+'/details'))
                    #main ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                    main_img_path = img_folder_path + '/main_img'
                    img_folder_path = self.make_dirs(main_img_path)
                else:
                    img_folder_path = img_folder_path = f"../data/{ctgr1}/{ctgr2}/{ctgr3}/{product_name}/{coupang_prod_id}"
                result = self.img_req(img_url,img_folder_path)
                #ìƒí’ˆ ë°ì´í„° ì •ë³´ tsv ì €ì¥
                if result:
                    with open(f'{self.item_info_file_name}.tsv','a',encoding='utf-8',newline='') as item_info_file:
                        writer2 = csv.writer(item_info_file,delimiter='\t')
                        #'ctgr1','ctgr2','ctgr3','product_name','main_img','item_img_path','dc_rate','price','tags_obj','detail_imgs'
                        writer2.writerow([ctgr1,ctgr2,ctgr3,product_name,main_img_path,img_folder_path,item['dc_rate'],item['price'],item['tags_obj'],img_folder_path+'/details'])
                else:
                    # ì‹¤íŒ¨ì‹œ ë§Œë“¤ì–´ ë†“ì€ ë””ë ‰í† ë¦¬ ì‚­ì œ
                    print("========================ì‹¤íŒ¨==========================",product_name)
                    os.rmdir(img_folder_path)

            if os.path.isdir(img_folder_path):
                ctgr_items = '>'.join([ctgr1,ctgr2,ctgr3])
                self.items_count[ctgr_items] += 1

    def change_word(self,word):
        word = word.replace(' ','')
        # íŠ¹ìˆ˜ë¬¸ì ë³€ê²½
        return re.sub('[-=+,#/\?:^$.@*\"â€»~&%ã†!ã€\\â€˜|\(\)\[\]\<\>`\'â€¦ã€‹Â·ğŸ’•â­]', '_', word)
    
    
    def img_req(self,img_url,img_folder_path,retries=0):
        file_name, _ = os.path.splitext(img_url)
        # file ì´ë¦„ jpg í™•ì¥ìë¡œ í†µì¼í•˜ê¸°ìœ„í•œ ì‘ì—…
        file_name = file_name.split('/')[-1].split('=')[-1]+'.jpg'
        try:
            result = urlretrieve(img_url,os.path.join(img_folder_path,file_name))
            # 3ë²ˆ ë” retry 
            if result == False and retries <= 3:
                print(result)
                time.sleep(3)
                retries +=1
                self.img_req(img_url,img_folder_path,file_name,retries)
            return result
        except HTTPError as e:
            if e.code == 404:
                #3ë²ˆ í–ˆëŠ”ë° ê³„ì† ë‚˜ì˜¤ëŠ” ê±´ no imageë¡œ íŒë‹¨. pass
                if retries > 3 : return print(f'[img_folder_path] : {img_folder_path} , [img_url] : {img_url}')
                time.sleep(3)
                retries +=1
                self.img_req(img_url,img_folder_path,file_name,retries)
            else:
                return None

    def make_dirs(self,path):
        if not os.path.exists(path):
            os.makedirs(path)
        return path
    
    def close_spider(self,spider):
        print('\n===== Crawling Summary =====')

        for key, val in self.items_count.items():
            print(f'  {key} category: {val} items.')