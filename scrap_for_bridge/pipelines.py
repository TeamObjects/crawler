# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter
from urllib.request import urlretrieve
from urllib.error import HTTPError
import os
import csv
import time
import re
import datetime
from .items import CtgrInfoItem, ProductInfoDetailItem
from collections import defaultdict

class ScrapForBridgePipeline:
    def __init__(self):
        self.start_time = datetime.date.today().strftime("%y%m%d")
        self.make_dirs('../data')
        self.items_count = defaultdict(int)
        self.ctgr_info_file_name = os.path.join("../data/", f'ì¹´í…Œê³ ë¦¬ì •ë³´_{self.start_time}')
        self.item_info_file_name = os.path.join("../data/", f'ìƒí’ˆì •ë³´_{self.start_time}')
        
        #ì¹´í…Œê³ ë¦¬ ì •ë³´ tsv íŒŒì¼ ìƒì„±
        with open(f'{self.ctgr_info_file_name}.tsv', 'a',encoding='utf-8', newline='') as ctgr_info_file:
            writer = csv.writer(ctgr_info_file, delimiter='\t')
            writer.writerow(['1ëìŠ¤ ì´ë¦„', '2ëìŠ¤ ì´ë¦„','3ëìŠ¤ ì´ë¦„','3ëìŠ¤ url'])
        #ìƒí’ˆ ì •ë³´ tsv íŒŒì¼ ìƒì„±
        with open(f'{self.item_info_file_name}.tsv','a',encoding='utf-8', newline='') as item_info_file:
                writer2 = csv.writer(item_info_file, delimiter='\t')
                writer2.writerow(['ctgr1','ctgr2','ctgr3','product_name','img_type','img_seq_no','dc_rate','price','tags_obj','img_path','option_info','img_url'])
        
    def process_item(self, item, spider):
        # ì¹´í…Œê³ ë¦¬ ì •ë³´ ë„˜ì–´ì˜¬ ë•Œ ì‹¤í–‰
        if isinstance(item,CtgrInfoItem):
            print(f'processing, {item["ctgr3_name"]}')
            with open(f'{self.ctgr_info_file_name}.tsv', 'a',encoding='utf-8', newline='') as ctgr_info_file:
                writer = csv.writer(ctgr_info_file, delimiter='\t')
                writer.writerow([item['ctgr1_name'], item['ctgr2_name'],item['ctgr3_name'],item['third_depth_url']])
        
        # ìƒí’ˆ ì •ë³´ ì €ì¥ íŒŒì´í”„ë¼ì¸
        if isinstance(item,ProductInfoDetailItem):
            print(f'processing, {item["product_name"]}')
            img_urls = item['item_img_urls']
            product_name = self.change_word(item['product_name'])
            ctgr1 = item['ctgr1']
            ctgr2 = item['ctgr2']
            ctgr3 = item['ctgr3']
            coupang_prod_id = re.sub(r' \- ','_',item['tags_obj']['ì¿ íŒ¡ìƒí’ˆë²ˆí˜¸'])
            # download path ì„¤ì • (os ê²½ë¡œ+ category_path + ìƒí’ˆ title.replace("/","_").replace(".","_") + ìƒí’ˆid)
            img_folder_path = f"../data/{ctgr1}/{ctgr2}/{ctgr3}/{product_name}/{coupang_prod_id}"
            # ìƒí’ˆ ì´ë¯¸ì§€ ì €ì¥ í´ë” ìƒì„±
            self.make_dirs(img_folder_path)
            xlxs_info = {}
            # ê³µí†µë°ì´í„° ì‘ì—…
            xlxs_info["ctgr1"] = ctgr1
            xlxs_info["ctgr2"] = ctgr2
            xlxs_info["ctgr3"] = ctgr3
            xlxs_info["product_name"] = product_name
            xlxs_info["img_folder_path"] = img_folder_path
            xlxs_info["dc_rate"] = item["dc_rate"]
            xlxs_info["price"] = item["price"]
            xlxs_info["tags_obj"] = item["tags_obj"]
            xlxs_info["option_info"] = item["option_info"]
            
            for i,img_url in enumerate(img_urls):
                #ìƒí’ˆ ìƒì„¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                for j,detail_url in enumerate(item['detail_urls']):
                    xlxs_info["img_type"] = 'detail'
                    xlxs_info["img_seq_no"] = j+1
                    detail_folder_path = f"../data/{ctgr1}/{ctgr2}/{ctgr3}/{product_name}/{coupang_prod_id}/details"
                    xlxs_info["img_file_name"] = detail_folder_path +'/'+ self.extract_filename(img_url)
                    xlxs_info["img_url"] = img_url
                    self.record_info(self.img_req(detail_url,self.make_dirs(detail_folder_path)),xlxs_info)
                    
                #ì‚¬ì§„ ìˆœì„œ
                xlxs_info["img_seq_no"] = i+1
                # main ì´ë¯¸ì§€ì¼ ê²½ìš° ê²½ë¡œ ë‹¤ë¥´ê²Œ í•˜ê¸°
                if i == 0 :
                    #main ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                    xlxs_info["img_type"] = 'main'
                    main_img_path = img_folder_path + '/main_img'
                    xlxs_info["img_file_name"] = main_img_path +'/'+ self.extract_filename(img_url)
                    img_folder_path = self.make_dirs(main_img_path)
                else:
                    xlxs_info["img_type"] = 'product'
                    img_folder_path = f"../data/{ctgr1}/{ctgr2}/{ctgr3}/{product_name}/{coupang_prod_id}"
                    xlxs_info["img_file_name"] = img_folder_path +'/'+ self.extract_filename(img_url)
                xlxs_info["img_url"] = img_url
                self.record_info(self.img_req(img_url,img_folder_path),xlxs_info)


            if os.path.isdir(img_folder_path):
                ctgr_items = '>'.join([ctgr1,ctgr2,ctgr3])
                self.items_count[ctgr_items] += 1

    def change_word(self,word):
        word = word.replace(' ','')
        # íŠ¹ìˆ˜ë¬¸ì ë³€ê²½
        return re.sub('[-=+,#/\?:^$.@*\"â€»~&%ã†!ã€\\â€˜|\(\)\[\]\<\>`\'â€¦ã€‹Â·ğŸ’•â­]', '_', word)
    
    def record_info(self,result,xlxs_info):
        ctgr1 = xlxs_info["ctgr1"]
        ctgr2 = xlxs_info["ctgr2"]
        ctgr3 = xlxs_info["ctgr3"]
        product_name = xlxs_info["product_name"]
        img_type = xlxs_info["img_type"]
        img_seq_no = xlxs_info["img_seq_no"]
        dc_rate = xlxs_info["dc_rate"]
        price = xlxs_info["price"]
        tags_obj = xlxs_info["tags_obj"]
        img_file_name = xlxs_info["img_file_name"]
        img_folder_path = xlxs_info["img_folder_path"]
        option_info = xlxs_info["option_info"]
        img_url = xlxs_info["img_url"]
        #ìƒí’ˆ ë°ì´í„° ì •ë³´ tsv ì €ì¥
        if result:
            with open(f'{self.item_info_file_name}.tsv','a',encoding='utf-8',newline='') as item_info_file:
                writer2 = csv.writer(item_info_file,delimiter='\t')
                #'ctgr1','ctgr2','ctgr3','product_name','img_type','img_seq_no','dc_rate','price','tags_obj','img_file_name'
                writer2.writerow([ctgr1,ctgr2,ctgr3,product_name,img_type,img_seq_no,dc_rate,price,tags_obj,img_file_name,option_info,img_url])
        else:
            # ì‹¤íŒ¨ì‹œ ë§Œë“¤ì–´ ë†“ì€ ë””ë ‰í† ë¦¬ ì‚­ì œ
            print("========================ì‹¤íŒ¨==========================",product_name)
            os.rmdir(img_folder_path)
        
    def img_req(self,img_url,img_folder_path,retries=0):
        file_name = self.extract_filename(img_url)
        try:
            result = urlretrieve(img_url,os.path.join(img_folder_path,file_name))
            # 3ë²ˆ ë” retry 
            if result == False and retries <= 3:
                print(result)
                time.sleep(3)
                retries +=1
                self.img_req(img_url,img_folder_path,file_name,retries)
            return result
        except HTTPError as e:
            if e.code == 404:
                #3ë²ˆ í–ˆëŠ”ë° ê³„ì† ë‚˜ì˜¤ëŠ” ê±´ no imageë¡œ íŒë‹¨. pass
                if retries > 3 : return print(f'[img_folder_path] : {img_folder_path} , [img_url] : {img_url}')
                time.sleep(3)
                retries +=1
                self.img_req(img_url,img_folder_path,file_name,retries)
            else:
                return None
            
    def extract_filename(self,img_url):
        file_name, _ = os.path.splitext(img_url)
        # file ì´ë¦„ jpg í™•ì¥ìë¡œ í†µì¼í•˜ê¸°ìœ„í•œ ì‘ì—…
        return file_name.split('/')[-1].split('=')[-1]+'.jpg'
        
        
    def make_dirs(self,path):
        if not os.path.exists(path):
            os.makedirs(path)
        return path
    
    def close_spider(self,spider):
        print('\n===== Crawling Summary =====')

        for key, val in self.items_count.items():
            print(f'  {key} category: {val} items.')